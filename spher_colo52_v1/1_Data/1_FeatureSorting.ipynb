{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/share/data/analyses/christa/colopaint3D_fork/spher_colo52_v1/1_Data'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import polars as pl # like pandas, but much faster\n",
    "import polars.selectors as cs\n",
    "import numpy as np\n",
    "import os, shutil, glob\n",
    "from random import randint\n",
    "import re, math\n",
    "import datetime\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sourceDir = '/share/data/cellprofiler/automation/results'\n",
    "rootDir = '/share/data/analyses/christa/colopaint3D_fork/spher_colo52_v1/1_Data/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 1)\n",
      "┌───────────────────┐\n",
      "│ feature_names     │\n",
      "│ ---               │\n",
      "│ str               │\n",
      "╞═══════════════════╡\n",
      "│ featICF_nuclei    │\n",
      "│ featICF_cells     │\n",
      "│ featICF_cytoplasm │\n",
      "└───────────────────┘\n"
     ]
    }
   ],
   "source": [
    "OutputDir = 'FeaturesImages_100125'\n",
    "NameContains = ''\n",
    "InputFolders = pl.read_csv(f'{rootDir}filemap.csv')\n",
    "print(InputFolders)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current date and time : \n",
      "2025-01-10 14:46:00\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "print ('Current date and time : ')\n",
    "print (now.strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['index','layout_id','cmpd_code', 'solvent','cmpd_conc_unit','stock_conc','stock_conc_unit','cmpd_vol', 'cmpd_vol_unit', 'well_vol', 'well_vol_unit', 'article_id','pubchemID', 'smiles', 'inkey', 'clinical_status']\n",
    "\n",
    "use_clipping = False\n",
    "\n",
    "std_mean = True\n",
    "\n",
    "make_slices = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObjectList = ['featICF_nuclei', 'featICF_cells', 'featICF_cytoplasm']\n",
    "\n",
    "def print_time(msg=None):\n",
    "    now = datetime.datetime.now()\n",
    "    print(now.strftime('%Y-%m-%d %H:%M:%S'),msg or \"\")\n",
    "float_columns=[pl.col(pl.Float32),pl.col(pl.Float64)]\n",
    "\n",
    "def aggregate_mean(df_in):\n",
    "    df_agg = df_in\n",
    "    df_float_columns=set(list(df_agg.select(float_columns).columns))\n",
    "    group_by_columns=['Metadata_PlateWell', 'Metadata_Barcode','Metadata_Well']\n",
    "    other_columns=set(list(df_agg.columns))-df_float_columns-set(group_by_columns)\n",
    "    # # group by mean for all float features, and group by first for all non-float columns (indices and string metadata)\n",
    "    group_by_aggregates=[\n",
    "        *[pl.mean(x) for x in list(df_float_columns)],\n",
    "        *[pl.first(x) for x in list(other_columns)]\n",
    "    ]\n",
    "    df_agg_mean=df_agg.group_by(group_by_columns).agg(group_by_aggregates)\n",
    "    return df_agg_mean\n",
    "\n",
    "def aggregate_median(df_in):\n",
    "    df_agg = df_in\n",
    "    df_float_columns=set(list(df_agg.select(float_columns).columns))\n",
    "    group_by_columns=['Metadata_Barcode','Metadata_Well']\n",
    "    other_columns=set(list(df_agg.columns))-df_float_columns-set(group_by_columns)\n",
    "    group_by_aggregates=[\n",
    "        *[pl.median(x) for x in list(df_float_columns)],\n",
    "        *[pl.first(x) for x in list(other_columns)]\n",
    "    ]\n",
    "    df_agg_median=df_agg.group_by(group_by_columns).agg(group_by_aggregates)\n",
    "    return df_agg_median\n",
    "\n",
    "def standardize_mean(df):\n",
    "    # df = df.with_row_count('index')\n",
    "    df_mean = pl.DataFrame()\n",
    "    for i in range(df.select(pl.col('Metadata_Site')).max().item()):\n",
    "        df_slice = df.filter(pl.col('Metadata_Site')==i)\n",
    "        df_slice_DMSO=df_slice.filter(pl.col('Metadata_cmpdname')=='dmso')\n",
    "        assert df_slice_DMSO.shape[0]>0, \"did not find any wells 'treated' with DMSO\"\n",
    "        mu = df_slice_DMSO.select(float_columns).mean()\n",
    "        std = df_slice_DMSO.select(float_columns).std()\n",
    "        # replace 0 with 1 (specifically not clip) to avoid div by zero\n",
    "        std = std.select([pl.col(c).replace({0: 1}, default=pl.col(c)) for c in std.columns])\n",
    "        for i,col in enumerate(std.columns):\n",
    "            if std[col].is_null().any():\n",
    "                raise RuntimeError(f\"some std value in column {col,i} is nan?!\")\n",
    "            if std[col].is_infinite().any():\n",
    "                raise RuntimeError(f\"some std value in column {col,i} is infinite?!\")\n",
    "            if (std[col]==0).any():\n",
    "                raise RuntimeError(f\"unexpected 0 in column {col}\")\n",
    "        print_time(\"calculated DMSO distribution for one slice\")\n",
    "        df_standardized_slice = df_slice.with_columns([(pl.col(c) - mu[c]) / (std[c]+0.01) for c in mu.columns])\n",
    "        found_nan=False\n",
    "        # checking nans:\n",
    "        for i,col in enumerate(mu.columns):\n",
    "            if df_standardized_slice[col].is_null().any():\n",
    "                found_nan=True\n",
    "                print(f\"some value in column {col,i} is nan\")\n",
    "        if found_nan:\n",
    "            raise RuntimeError(\"found nan\")\n",
    "        df_mean_slice=df_slice.with_columns([df_standardized_slice[c] for c in df_standardized_slice.columns])   \n",
    "        df_mean = pl.concat([df_mean, df_mean_slice])\n",
    "    # df_mean\n",
    "    return df_mean\n",
    "\n",
    "def standardize_mean_perplate(df):\n",
    "    plate_list = df.select(pl.col('Metadata_Barcode')).to_series().unique().to_list()\n",
    "    df_mean = pl.DataFrame()\n",
    "    print(plate_list)\n",
    "    for plate in plate_list:\n",
    "        print(f'processing barcode {plate}')\n",
    "        df_set = df.filter(pl.col('Metadata_Barcode')==plate)\n",
    "        df_set = standardize_mean(df_set)\n",
    "        df_mean = pl.concat([df_mean, df_set])\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize(df):\n",
    "    df = df.with_row_count('index')\n",
    "    df_norm = pl.DataFrame()\n",
    "    for i in range(df.select(pl.col('Metadata_Site')).max().item()):\n",
    "        df_slice = df.filter(pl.col('Metadata_Site')==i)\n",
    "        df_slice_DMSO=df_slice.filter(pl.col('Metadata_cmpdname')=='dmso') # @DAVID TODO: sometimes dmso is capitalized\n",
    "        assert df_slice_DMSO.shape[0]>0, \"did not find any wells 'treated' with DMSO\"\n",
    "        maxi = df_slice_DMSO.select(float_columns).max()\n",
    "        mini = df_slice_DMSO.select(float_columns).min()\n",
    "        # replace 0 with 1 (specifically not clip) to avoid div by zero\n",
    "        maxi = maxi.select([pl.col(c).replace({0: 1}, default=pl.col(c)) for c in maxi.columns])\n",
    "        for i,col in enumerate(mini.columns):\n",
    "            if maxi[col].is_null().any():\n",
    "                raise RuntimeError(f\"some std value in column {col,i} is nan?!\")\n",
    "            if maxi[col].is_infinite().any():\n",
    "                raise RuntimeError(f\"some std value in column {col,i} is infinite?!\")\n",
    "            if (maxi[col]==0).any():\n",
    "                raise RuntimeError(f\"unexpected 0 in column {col}\")\n",
    "        print_time(\"calculated DMSO distribution for one slice\")\n",
    "        df_normalized_slice = df_slice.with_columns([(pl.col(c) - mini[c]) / (maxi[c]) for c in maxi.columns])\n",
    "        found_nan=False\n",
    "        # checking nans:\n",
    "        for i,col in enumerate(maxi.columns):\n",
    "            if df_normalized_slice[col].is_null().any():\n",
    "                found_nan=True\n",
    "                print(f\"some value in column {col,i} is nan\")\n",
    "        if found_nan:\n",
    "            raise RuntimeError(\"found nan\")\n",
    "        df_norm_slice=df_slice.with_columns([df_normalized_slice[c] for c in df_normalized_slice.columns])   \n",
    "        df_norm = pl.concat([df_norm, df_norm_slice])\n",
    "    # df_mean\n",
    "    return df_norm\n",
    "\n",
    "def normalize_perplate(df):\n",
    "    plate_list = df.select(pl.col('Metadata_Barcode')).to_series().unique().to_list()\n",
    "    print(plate_list)\n",
    "    df_mean = pl.DataFrame()\n",
    "    for plate in plate_list:\n",
    "        print(f'processing barcode {plate}')\n",
    "        df_set = df.filter(pl.col('Metadata_Barcode')==plate)\n",
    "        df_set = normalize(df_set)\n",
    "        df_mean = pl.concat([df_mean, df_set])\n",
    "    return df\n",
    "\n",
    "def generate_slices(df, outdir, arg):\n",
    "    df_sAgg = pl.DataFrame()\n",
    "    for i in range(df.select(pl.col('Metadata_Site')).max().item()):\n",
    "        df_slice = df.filter(pl.col('Metadata_Site')==i)\n",
    "        if arg == 'mean':\n",
    "            df_slice_agg = aggregate_mean(df_slice)\n",
    "            df_slice_agg.write_parquet(f'{outdir}/{df.select(pl.col(\"Metadata_cell_line\")).unique().item()}_Slice{i}MeanAgg.parquet')\n",
    "        elif arg == 'median':\n",
    "            df_slice_agg = aggregate_median(df_slice)\n",
    "            df_slice_agg.write_parquet(f'{outdir}/{df.select(pl.col(\"Metadata_cell_line\")).unique().item()}_Slice{i}MedianAgg.parquet')\n",
    "        else:\n",
    "            print('ERROR no valid metric')\n",
    "        del df_slice\n",
    "        df_sAgg = pl.concat([df_sAgg, df_slice_agg])\n",
    "    return df_sAgg\n",
    "\n",
    "def data_processing(metaEx, cl, ObjectList, OutputDir, sliceLim=13, statmet='minmax', per_plate=False):\n",
    "    OutputDir = f'{OutputDir}_{statmet}'\n",
    "    if per_plate:\n",
    "        OutputDir = f'{OutputDir}_PerPlate'\n",
    "    #Filtering Metadata and generating dirlist\n",
    "    metaEx = metaEx.filter(pl.col('cell_line')==cl)\n",
    "    barcodes = metaEx.select(pl.col('barcode')).unique()\n",
    "    barcodes = barcodes['barcode']\n",
    "    barcodes.to_list()\n",
    "    dirlist = [f'{sourceDir}/{barcode}' for barcode in barcodes]\n",
    "\n",
    "    print('Starting Processing')\n",
    "    df = pl.DataFrame()\n",
    "    for BaseDir in dirlist:\n",
    "        mdf_op = metaEx.filter(pl.col('barcode') == BaseDir.split('/')[-1])\n",
    "        image_id = mdf_op.select(pl.col('image_id')).unique().to_series().to_list()[-1]\n",
    "        cp_id = mdf_op.select(pl.col('cp_id')).unique().to_series().to_list()[-1]\n",
    "        print(f'{BaseDir}/{image_id}/{cp_id}')\n",
    "        BaseDir = f'{BaseDir}/{image_id}/{cp_id}'      \n",
    "\n",
    "        nuclei = pl.read_parquet(BaseDir+f'/{ObjectList[0]}.parquet')\n",
    "        nuclei=nuclei.rename({x:f'{x}_nuclei' for x in nuclei.columns})\n",
    "        cytoplasm = pl.read_parquet(BaseDir+f'/{ObjectList[1]}.parquet')\n",
    "        cytoplasm=cytoplasm.rename({x:f'{x}_cytoplasm' for x in cytoplasm.columns}) \n",
    "        cells = pl.read_parquet(BaseDir+f'/{ObjectList[2]}.parquet')\n",
    "        cells=cells.rename({x:f'{x}_cells' for x in cells.columns})\n",
    "        # step 1: Take the mean values of 'multiple nuclei' belonging to one cell\n",
    "\n",
    "        nuclei = nuclei.group_by([\n",
    "            \"Metadata_Barcode_nuclei\",\"Metadata_Well_nuclei\",\n",
    "            \"Parent_cells_nuclei\", 'Metadata_Site_nuclei'\n",
    "        ]).mean()\n",
    "\n",
    "        df_one = cytoplasm.join(nuclei,\n",
    "                    how='left', \n",
    "                    right_on=['Metadata_Well_nuclei', 'Metadata_Site_nuclei', 'Parent_cells_nuclei', 'Metadata_Barcode_nuclei'],\n",
    "                    left_on = ['Metadata_Well_cytoplasm','Metadata_Site_cytoplasm', 'ObjectNumber_cytoplasm', 'Metadata_Barcode_cytoplasm'])\n",
    "                    \n",
    "        df_one = df_one.join(cells, how='left', \n",
    "                        left_on=['Metadata_Well_cytoplasm','Metadata_Site_cytoplasm','ObjectNumber_cytoplasm', 'Metadata_Barcode_cytoplasm'],\n",
    "                        right_on = ['Metadata_Well_cells','Metadata_Site_cells',\"ObjectNumber_cells\", 'Metadata_Barcode_cells'])\n",
    "\n",
    "        # print_time(\"initial merging\")\n",
    "        print('part1')\n",
    "\n",
    "\n",
    "\n",
    "        # deduplicate barcode/well/site - renamed cytoplasm_Metadata* to Metadata* and removes nuclei_* etc\n",
    "        unique_metadata_feature_names=['Metadata_Barcode','Metadata_Well','Metadata_Site']\n",
    "        df_one=df_one.rename({f'{suffix}_cytoplasm':suffix for suffix in unique_metadata_feature_names})   ##TODO: Change this here. \n",
    "        # df = df.filter(pl.col(''))       \n",
    "        # for some reason, the site is parsed as float, even though it really should be an int\n",
    "        if df_one['Metadata_Site'].dtype in [np.dtype('float32'), np.dtype('float64')]:\n",
    "            # sometimes, for some reason, site indices are inf/nan\n",
    "            site_is_nan_mask=np.isnan(df_one['Metadata_Site'])\n",
    "            site_is_inf_mask=np.isinf(df_one['Metadata_Site'])\n",
    "            \n",
    "            try:\n",
    "                num_sites_nan=np.sum(site_is_nan_mask)\n",
    "                num_sites_inf=np.sum(site_is_inf_mask)\n",
    "                assert num_sites_nan==0, f\"found nan site values (n = {num_sites_nan})\"\n",
    "                assert num_sites_inf==0, f\"found inf site values (n = {num_sites_inf})\"\n",
    "            except AssertionError as e:\n",
    "                print(f\"info - this issue was automatically circumvented in the code : {e}\")\n",
    "                df_one=df_one[~(site_is_inf_mask|site_is_nan_mask)]\n",
    "                \n",
    "            num_metadata_site_entries_nonint=np.sum(np.abs(df_one['Metadata_Site']%1.0)>1e-6)\n",
    "            assert num_metadata_site_entries_nonint==0, f\"ERROR : {num_metadata_site_entries_nonint} imaging sites don't have integer indices. that should not be the case, and likely indicates a bug.\"\n",
    "            \n",
    "            #Should use np.round, no? TODO ask patrick. Truncation Errors are annoying.\n",
    "            df_one['Metadata_Site'] = df_one['Metadata_Site'].astype(np.dtype('int32'))\n",
    "        \n",
    "        #Adding Compound Metadata to each row\n",
    "        df_one = df_one.join(mdf_op.rename({x:f\"Metadata_{x}\" for x in mdf_op.columns}),left_on='Metadata_Well',right_on='Metadata_well_id')\n",
    "        df_one = df_one.filter(pl.col('Metadata_Site')<sliceLim)\n",
    "        df = pl.concat([df, df_one])\n",
    "        plate_name = f'processed metadata for {BaseDir.split(\"/\")[-1]}'\n",
    "        print(plate_name)\n",
    "\n",
    "    df = df.sort(pl.col('Metadata_Site'))\n",
    "    ###Here should be workable to unify by cell line.\n",
    "    df = df.with_columns((pl.col(\"Metadata_Barcode\") + \"_\" + pl.col(\"Metadata_Well\")).alias(\"Metadata_PlateWell\"))\n",
    "    print(df.select(pl.col('Metadata_cell_line')).to_series().unique().to_list())\n",
    "    df = df.filter(pl.col('Metadata_cell_line')==cl)\n",
    "    print(df.select(pl.col('Metadata_cell_line')).to_series().unique().to_list())\n",
    "    ###End\n",
    "    # drop all rows that contain nan\n",
    "    num_rows_before_nan_trim = df.shape[0]\n",
    "    for col in df.select([pl.col(pl.Float32),pl.col(pl.Float64)]).columns:\n",
    "        before_drop=df.shape[0]\n",
    "        df=df.filter(pl.col(col).is_not_null())\n",
    "        after_drop=df.shape[0]\n",
    "\n",
    "        num_values_dropped=before_drop-after_drop\n",
    "        if num_values_dropped>0:\n",
    "           print(f\"dropped {num_values_dropped} rows due to NaNs in column {col}\")\n",
    "\n",
    "    num_rows_after_nan_trim = df.shape[0]\n",
    "    print_time(\"dropped NaNs\")\n",
    "    print(num_rows_after_nan_trim)\n",
    "    # Clip outliers\n",
    "    use_clipping = False\n",
    "    if use_clipping:\n",
    "        print('clipping values....')\n",
    "        float_cols = [c for c_name,c_dtype in zip(df.columns,df.dtypes) if \"float\" in str(c_dtype)]\n",
    "        lower_quantile = df.select(float_cols).quantile(0.01)\n",
    "        upper_quantile = df.select(float_cols).quantile(0.99)\n",
    "        print(\"calced quantiles\")\n",
    "        for col in float_cols:\n",
    "            df = df.with_column(\n",
    "                pl.col(col).clip(lower=lower_quantile[col],upper=upper_quantile[col])\n",
    "            )\n",
    "        print(\"clipped\")\n",
    "\n",
    "\n",
    "    # # # df_mean=df.with_columns([df_standardized[c] for c in df_standardized.columns])\n",
    "    print(per_plate)\n",
    "    if statmet == 'meanstd':\n",
    "        print('Standardizing by MeanSTD')\n",
    "        if per_plate:\n",
    "            print('standardizing per plate')\n",
    "            df = standardize_mean_perplate(df)\n",
    "        else:\n",
    "            df = standardize_mean(df)\n",
    "    if statmet == 'minmax':\n",
    "        print('Normalizing')\n",
    "        if per_plate:\n",
    "            print('normalizing per plate')\n",
    "            df = normalize_perplate(df)\n",
    "        else:\n",
    "            df = normalize(df)\n",
    "    # df_mean = df\n",
    "\n",
    "    # Remove unnecessary columns\n",
    "    pattern = re.compile(r'FileName|PathName|ObjectNumber|ImageNumber|AcqID')\n",
    "    # pattern = re.compile(r'FileName|PathName')\n",
    "    columns_to_keep = [col for col in df.columns if not pattern.search(col)]\n",
    "    dfOut = df.select(columns_to_keep)\n",
    "    \n",
    "    ScOut = f'{OutputDir}/SingleCell/'\n",
    "    if not os.path.exists(ScOut): \n",
    "        os.makedirs(ScOut)\n",
    "    df.write_parquet(f'{ScOut}/{dfOut.select(pl.col(\"Metadata_cell_line\")).unique().item()}.parquet')\n",
    "\n",
    "    # if make_slices:\n",
    "    slicesOut = f'{OutputDir}/SingleSlice/'\n",
    "    if not os.path.exists(slicesOut): \n",
    "        os.makedirs(slicesOut)\n",
    "    df = generate_slices(dfOut, slicesOut, 'median')\n",
    "    \n",
    "\n",
    "    #Generating the output directories\n",
    "    aggOut = f'{OutputDir}/WellAggregates/'\n",
    "    if not os.path.exists(aggOut): \n",
    "        os.makedirs(aggOut)\n",
    "    \n",
    "\n",
    "    # df_agg_mean = aggregate_mean(df)\n",
    "    # df_agg_mean.write_parquet(f'{aggOut}/{dfOut.select(pl.col(\"Metadata_cell_line\")).unique().item()}_MeanAgg_meanstd.parquet')\n",
    "    df_agg_median = aggregate_median(dfOut)\n",
    "    df_agg_median.write_parquet(f'{aggOut}/{dfOut.select(pl.col(\"Metadata_cell_line\")).unique().item()}_MedianAgg_meanstd.parquet')\n",
    "    del df_agg_median\n",
    "    # del df_agg_mean\n",
    "    # del df_mean\n",
    "    print_time(\"binned mean data per well\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaEx = pl.read_csv(f'{rootDir}spher_colo52-metadata.csv')\n",
    "metaEx = metaEx.drop(cols_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>well_id</th><th>image_id</th><th>cp_id</th><th>barcode</th><th>plate_well</th><th>cmpdname</th><th>cmpd_conc</th><th>pert_type</th><th>target</th><th>pathway</th><th>target_type</th><th>cell_line</th></tr><tr><td>str</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;B02&quot;</td><td>4185</td><td>5532</td><td>&quot;PB000137&quot;</td><td>&quot;PB000137_B02&quot;</td><td>&quot;PD0325901&quot;</td><td>3.0</td><td>&quot;trt&quot;</td><td>&quot;MEK&quot;</td><td>&quot;MAPK&quot;</td><td>&quot;Targeted&quot;</td><td>&quot;HCT116&quot;</td></tr><tr><td>&quot;B03&quot;</td><td>4185</td><td>5532</td><td>&quot;PB000137&quot;</td><td>&quot;PB000137_B03&quot;</td><td>&quot;Paclitaxel&quot;</td><td>0.1</td><td>&quot;trt&quot;</td><td>&quot;Autophagy,Micr…</td><td>&quot;Cytoskeletal S…</td><td>&quot;Cytotoxic&quot;</td><td>&quot;HCT116&quot;</td></tr><tr><td>&quot;B04&quot;</td><td>4185</td><td>5532</td><td>&quot;PB000137&quot;</td><td>&quot;PB000137_B04&quot;</td><td>&quot;Olaparib (AZD2…</td><td>3.0</td><td>&quot;trt&quot;</td><td>&quot;PARP&quot;</td><td>&quot;DNA Damage&quot;</td><td>&quot;Targeted&quot;</td><td>&quot;HCT116&quot;</td></tr><tr><td>&quot;B05&quot;</td><td>4185</td><td>5532</td><td>&quot;PB000137&quot;</td><td>&quot;PB000137_B05&quot;</td><td>&quot;SB216763&quot;</td><td>10.0</td><td>&quot;trt&quot;</td><td>&quot;GSK-3&quot;</td><td>&quot;PI3K/Akt/mTOR&quot;</td><td>&quot;Targeted&quot;</td><td>&quot;HCT116&quot;</td></tr><tr><td>&quot;B06&quot;</td><td>4185</td><td>5532</td><td>&quot;PB000137&quot;</td><td>&quot;PB000137_B06&quot;</td><td>&quot;Vorinostat (SA…</td><td>3.0</td><td>&quot;trt&quot;</td><td>&quot;Autophagy,HDAC…</td><td>&quot;Epigenetics&quot;</td><td>&quot;Targeted&quot;</td><td>&quot;HCT116&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 12)\n",
       "┌─────────┬──────────┬───────┬──────────┬───┬──────────────┬─────────────┬─────────────┬───────────┐\n",
       "│ well_id ┆ image_id ┆ cp_id ┆ barcode  ┆ … ┆ target       ┆ pathway     ┆ target_type ┆ cell_line │\n",
       "│ ---     ┆ ---      ┆ ---   ┆ ---      ┆   ┆ ---          ┆ ---         ┆ ---         ┆ ---       │\n",
       "│ str     ┆ i64      ┆ i64   ┆ str      ┆   ┆ str          ┆ str         ┆ str         ┆ str       │\n",
       "╞═════════╪══════════╪═══════╪══════════╪═══╪══════════════╪═════════════╪═════════════╪═══════════╡\n",
       "│ B02     ┆ 4185     ┆ 5532  ┆ PB000137 ┆ … ┆ MEK          ┆ MAPK        ┆ Targeted    ┆ HCT116    │\n",
       "│ B03     ┆ 4185     ┆ 5532  ┆ PB000137 ┆ … ┆ Autophagy,Mi ┆ Cytoskeleta ┆ Cytotoxic   ┆ HCT116    │\n",
       "│         ┆          ┆       ┆          ┆   ┆ crotubule    ┆ l Signaling ┆             ┆           │\n",
       "│         ┆          ┆       ┆          ┆   ┆ Associated   ┆             ┆             ┆           │\n",
       "│ B04     ┆ 4185     ┆ 5532  ┆ PB000137 ┆ … ┆ PARP         ┆ DNA Damage  ┆ Targeted    ┆ HCT116    │\n",
       "│ B05     ┆ 4185     ┆ 5532  ┆ PB000137 ┆ … ┆ GSK-3        ┆ PI3K/Akt/mT ┆ Targeted    ┆ HCT116    │\n",
       "│         ┆          ┆       ┆          ┆   ┆              ┆ OR          ┆             ┆           │\n",
       "│ B06     ┆ 4185     ┆ 5532  ┆ PB000137 ┆ … ┆ Autophagy,HD ┆ Epigenetics ┆ Targeted    ┆ HCT116    │\n",
       "│         ┆          ┆       ┆          ┆   ┆ AC           ┆             ┆             ┆           │\n",
       "└─────────┴──────────┴───────┴──────────┴───┴──────────────┴─────────────┴─────────────┴───────────┘"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metaEx.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metaEx.select(pl.col(['cell_line'])).unique().to_series().to_list()\n",
    "# metaEx.select(pl.col(['cell_line'])).unique().to_series().to_list()\n",
    "# metaEx.select(pl.col(['barcode'])).unique().to_series().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HCT116', 'HT29']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metaEx.select(pl.col(['cell_line'])).unique().to_series().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_processing(metaEx, 'HCT116', ObjectList, OutputDir, statmet='meanstd', per_plate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_processing(metaEx, 'HCT116', ObjectList, OutputDir, statmet='meanstd', per_plate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_processing(metaEx, 'HT29', ObjectList, OutputDir, statmet='meanstd', per_plate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PB000138', 'PB000137', 'PB000139', 'PB000140', 'PB000141', 'PB000142']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metaEx.select(pl.col(['barcode'])).unique().to_series().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_processing(metaEx, 'HCT116', ObjectList, OutputDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_processing(metaEx, 'HCT116', ObjectList, OutputDir, statmet='minmax', per_plate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_processing(metaEx, 'HCT116', ObjectList, OutputDir, statmet='meanstd', per_plate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_processing(metaEx, 'HCT116', ObjectList, OutputDir, statmet='meanstd', per_plate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Processing\n",
      "/share/data/cellprofiler/automation/results/PB000139/4186/5533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part1\n",
      "processed metadata for 5533\n",
      "/share/data/cellprofiler/automation/results/PB000138/4188/5547\n",
      "part1\n",
      "processed metadata for 5547\n",
      "/share/data/cellprofiler/automation/results/PB000140/4189/5573\n",
      "part1\n",
      "processed metadata for 5573\n",
      "/share/data/cellprofiler/automation/results/PB000137/4185/5532\n",
      "part1\n",
      "processed metadata for 5532\n",
      "['HCT116']\n",
      "['HCT116']\n",
      "dropped 1 rows due to NaNs in column AreaShape_FormFactor_cytoplasm\n",
      "dropped 3 rows due to NaNs in column Correlation_Costes_CONC_HOECHST_cytoplasm\n",
      "dropped 1 rows due to NaNs in column Correlation_Costes_MITO_HOECHST_cytoplasm\n",
      "dropped 25 rows due to NaNs in column Neighbors_AngleBetweenNeighbors_Adjacent_cytoplasm\n",
      "dropped 4 rows due to NaNs in column Neighbors_FirstClosestDistance_Adjacent_cytoplasm\n",
      "dropped 36 rows due to NaNs in column Neighbors_AngleBetweenNeighbors_Adjacent_nuclei\n",
      "dropped 4 rows due to NaNs in column Neighbors_FirstClosestDistance_Adjacent_nuclei\n",
      "dropped 30 rows due to NaNs in column AreaShape_Area_cells\n",
      "dropped 11 rows due to NaNs in column AreaShape_FormFactor_cells\n",
      "dropped 28 rows due to NaNs in column AreaShape_Zernike_0_0_cells\n",
      "2025-01-10 14:46:54 dropped NaNs\n",
      "873550\n",
      "False\n",
      "2025-01-10 14:48:29 binned mean data per well\n"
     ]
    }
   ],
   "source": [
    "data_processing(metaEx, 'HCT116', ObjectList, OutputDir, statmet='none', per_plate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_processing(metaEx, 'HT29', ObjectList, OutputDir, statmet='minmax', per_plate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_processing(metaEx, 'HT29', ObjectList, OutputDir, statmet='minmax', per_plate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_processing(metaEx, 'HT29', ObjectList, OutputDir, statmet='meanstd', per_plate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_processing(metaEx, 'HT29', ObjectList, OutputDir, statmet='meanstd', per_plate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_processing(metaEx, 'HT29', ObjectList, OutputDir, statmet='none', per_plate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current date and time : \n",
      "2025-01-10 14:48:30\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "print ('Current date and time : ')\n",
    "print (now.strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5574"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metaEx.select(pl.col('cp_id')).max().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
