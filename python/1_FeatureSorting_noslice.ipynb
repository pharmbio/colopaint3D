{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch2-shared/david/colopaint3D/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import polars as pl # like pandas, but much faster\n",
    "import polars.selectors as cs\n",
    "import numpy as np\n",
    "import os, shutil, glob\n",
    "from random import randint\n",
    "import re, math\n",
    "import datetime\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sourceDir = '/share/data/cellprofiler/automation/results'\n",
    "rootDir = '/home/jovyan/scratch2-shared/david/colopaint3D'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 1)\n",
      "┌───────────────────┐\n",
      "│ feature_names     │\n",
      "│ ---               │\n",
      "│ str               │\n",
      "╞═══════════════════╡\n",
      "│ featICF_nuclei    │\n",
      "│ featICF_cells     │\n",
      "│ featICF_cytoplasm │\n",
      "└───────────────────┘\n"
     ]
    }
   ],
   "source": [
    "OutputDir = 'data/1_FeaturesImages'\n",
    "if not os.path.exists(OutputDir): \n",
    "    os.makedirs(OutputDir)\n",
    "NameContains = ''\n",
    "InputFolders = pl.read_csv('../settings/filemap.csv')\n",
    "print(InputFolders )\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current date and time : \n",
      "2024-10-24 10:07:06\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "print ('Current date and time : ')\n",
    "print (now.strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['index','layout_id','cmpd_code', 'solvent','cmpd_conc','cmpd_conc_unit','stock_conc','stock_conc_unit','cmpd_vol', 'cmpd_vol_unit', 'well_vol', 'well_vol_unit', 'article_id','pubchemID', 'smiles', 'inkey', 'clinical_status']\n",
    "\n",
    "use_clipping = False\n",
    "\n",
    "std_mean = True\n",
    "\n",
    "make_slices = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ObjectList = ['featICF_nuclei', 'featICF_cells', 'featICF_cytoplasm']\n",
    "\n",
    "def print_time(msg=None):\n",
    "    now = datetime.datetime.now()\n",
    "    print(now.strftime('%Y-%m-%d %H:%M:%S'),msg or \"\")\n",
    "float_columns=[pl.col(pl.Float32),pl.col(pl.Float64)]\n",
    "\n",
    "def aggregate_mean(df_in):\n",
    "    df_agg = df_in\n",
    "    df_float_columns=set(list(df_agg.select(float_columns).columns))\n",
    "    group_by_columns=['Metadata_PlateWell', 'Metadata_Barcode','Metadata_Well']\n",
    "    other_columns=set(list(df_agg.columns))-df_float_columns-set(group_by_columns)\n",
    "    # # group by mean for all float features, and group by first for all non-float columns (indices and string metadata)\n",
    "    group_by_aggregates=[\n",
    "        *[pl.mean(x) for x in list(df_float_columns)],\n",
    "        *[pl.first(x) for x in list(other_columns)]\n",
    "    ]\n",
    "    df_agg_mean=df_agg.group_by(group_by_columns).agg(group_by_aggregates)\n",
    "    return df_agg_mean\n",
    "\n",
    "def aggregate_median(df_in):\n",
    "    df_agg = df_in\n",
    "    df_float_columns=set(list(df_agg.select(float_columns).columns))\n",
    "    group_by_columns=['Metadata_Barcode','Metadata_Well']\n",
    "    other_columns=set(list(df_agg.columns))-df_float_columns-set(group_by_columns)\n",
    "    group_by_aggregates=[\n",
    "        *[pl.median(x) for x in list(df_float_columns)],\n",
    "        *[pl.first(x) for x in list(other_columns)]\n",
    "    ]\n",
    "    df_agg_median=df_agg.group_by(group_by_columns).agg(group_by_aggregates)\n",
    "    return df_agg_median\n",
    "\n",
    "def standardize_mean(df):\n",
    "    # df = df.with_row_count('index')\n",
    "    df_mean = pl.DataFrame()\n",
    "    for i in range(df.select(pl.col('Metadata_Site')).max().item()):\n",
    "        df_slice = df.filter(pl.col('Metadata_Site')==i)\n",
    "        df_slice_DMSO=df_slice.filter(pl.col('Metadata_cmpd_cmpdname')=='dmso')\n",
    "        assert df_slice_DMSO.shape[0]>0, \"did not find any wells 'treated' with DMSO\"\n",
    "        mu = df_slice_DMSO.select(float_columns).mean()\n",
    "        std = df_slice_DMSO.select(float_columns).std()\n",
    "        # replace 0 with 1 (specifically not clip) to avoid div by zero\n",
    "        std = std.select([pl.col(c).replace({0: 1}, default=pl.col(c)) for c in std.columns])\n",
    "        for i,col in enumerate(std.columns):\n",
    "            if std[col].is_null().any():\n",
    "                raise RuntimeError(f\"some std value in column {col,i} is nan?!\")\n",
    "            if std[col].is_infinite().any():\n",
    "                raise RuntimeError(f\"some std value in column {col,i} is infinite?!\")\n",
    "            if (std[col]==0).any():\n",
    "                raise RuntimeError(f\"unexpected 0 in column {col}\")\n",
    "        print_time(\"calculated DMSO distribution for one slice\")\n",
    "        df_standardized_slice = df_slice.with_columns([(pl.col(c) - mu[c]) / (std[c]+0.01) for c in mu.columns])\n",
    "        found_nan=False\n",
    "        # checking nans:\n",
    "        for i,col in enumerate(mu.columns):\n",
    "            if df_standardized_slice[col].is_null().any():\n",
    "                found_nan=True\n",
    "                print(f\"some value in column {col,i} is nan\")\n",
    "        if found_nan:\n",
    "            raise RuntimeError(\"found nan\")\n",
    "        df_mean_slice=df_slice.with_columns([df_standardized_slice[c] for c in df_standardized_slice.columns])   \n",
    "        df_mean = pl.concat([df_mean, df_mean_slice])\n",
    "    # df_mean\n",
    "    return df_mean\n",
    "\n",
    "def standardize_mean_noslice(df):\n",
    "    df_DMSO=df.filter(pl.col('Metadata_cmpd_cmpdname')=='dmso')\n",
    "    assert df_DMSO.shape[0]>0, \"did not find any wells 'treated' with DMSO\"\n",
    "    mu = df_DMSO.select(float_columns).mean()\n",
    "    std = df_DMSO.select(float_columns).std()\n",
    "    # replace 0 with 1 (specifically not clip) to avoid div by zero\n",
    "    std = std.select([pl.col(c).replace({0: 1}, default=pl.col(c)) for c in std.columns])\n",
    "    for i,col in enumerate(std.columns):\n",
    "        if std[col].is_null().any():\n",
    "            raise RuntimeError(f\"some std value in column {col,i} is nan?!\")\n",
    "        if std[col].is_infinite().any():\n",
    "            raise RuntimeError(f\"some std value in column {col,i} is infinite?!\")\n",
    "        if (std[col]==0).any():\n",
    "            raise RuntimeError(f\"unexpected 0 in column {col}\")\n",
    "\n",
    "    print_time(\"calculated DMSO distribution\")\n",
    "    df_standardized = df.with_columns([(pl.col(c) - mu[c]) / (std[c]+0.01) for c in mu.columns])\n",
    "    found_nan=False\n",
    "    # checking nans:\n",
    "    for i,col in enumerate(mu.columns):\n",
    "        if df_standardized[col].is_null().any():\n",
    "            found_nan=True\n",
    "            print(f\"some value in column {col,i} is nan\")\n",
    "\n",
    "    if found_nan:\n",
    "        raise RuntimeError(\"found nan\")\n",
    "    df_mean=df.with_columns([df_standardized[c] for c in df_standardized.columns])\n",
    "    return df_mean\n",
    "\n",
    "\n",
    "def standardize_mean_perplate(df):\n",
    "    plate_list = df.select(pl.col('Metadata_Barcode')).to_series().unique().to_list()\n",
    "    df_mean = pl.DataFrame()\n",
    "    print(plate_list)\n",
    "    for plate in plate_list:\n",
    "        print(f'processing barcode {plate}')\n",
    "        df_set = df.filter(pl.col('Metadata_Barcode')==plate)\n",
    "        df_set = standardize_mean_noslice(df_set)\n",
    "        df_mean = pl.concat([df_mean, df_set])\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize(df):\n",
    "    df = df.with_row_count('index')\n",
    "    df_norm = pl.DataFrame()\n",
    "    for i in range(df.select(pl.col('Metadata_Site')).max().item()):\n",
    "        df_slice = df.filter(pl.col('Metadata_Site')==i)\n",
    "        df_slice_DMSO=df_slice.filter(pl.col('Metadata_cmpd_cmpdname')=='dmso')\n",
    "        assert df_slice_DMSO.shape[0]>0, \"did not find any wells 'treated' with DMSO\"\n",
    "        maxi = df_slice_DMSO.select(float_columns).max()\n",
    "        mini = df_slice_DMSO.select(float_columns).min()\n",
    "        # replace 0 with 1 (specifically not clip) to avoid div by zero\n",
    "        maxi = maxi.select([pl.col(c).replace({0: 1}, default=pl.col(c)) for c in maxi.columns])\n",
    "        for i,col in enumerate(mini.columns):\n",
    "            if maxi[col].is_null().any():\n",
    "                raise RuntimeError(f\"some std value in column {col,i} is nan?!\")\n",
    "            if maxi[col].is_infinite().any():\n",
    "                raise RuntimeError(f\"some std value in column {col,i} is infinite?!\")\n",
    "            if (maxi[col]==0).any():\n",
    "                raise RuntimeError(f\"unexpected 0 in column {col}\")\n",
    "        print_time(\"calculated DMSO distribution for one slice\")\n",
    "        df_normalized_slice = df_slice.with_columns([(pl.col(c) - mini[c]) / (maxi[c]) for c in maxi.columns])\n",
    "        found_nan=False\n",
    "        # checking nans:\n",
    "        for i,col in enumerate(maxi.columns):\n",
    "            if df_normalized_slice[col].is_null().any():\n",
    "                found_nan=True\n",
    "                print(f\"some value in column {col,i} is nan\")\n",
    "        if found_nan:\n",
    "            raise RuntimeError(\"found nan\")\n",
    "        df_norm_slice=df_slice.with_columns([df_normalized_slice[c] for c in df_normalized_slice.columns])   \n",
    "        df_norm = pl.concat([df_norm, df_norm_slice])\n",
    "    # df_mean\n",
    "    return df_norm\n",
    "\n",
    "\n",
    "\n",
    "def normalize_perplate(df):\n",
    "    plate_list = df.select(pl.col('Metadata_Barcode')).to_series().unique().to_list()\n",
    "    print(plate_list)\n",
    "    df_mean = pl.DataFrame()\n",
    "    for plate in plate_list:\n",
    "        print(f'processing barcode {plate}')\n",
    "        df_set = df.filter(pl.col('Metadata_Barcode')==plate)\n",
    "        df_set = normalize(df_set)\n",
    "        df_mean = pl.concat([df_mean, df_set])\n",
    "    return df\n",
    "\n",
    "def generate_slices(df, outdir, arg):\n",
    "    df_sAgg = pl.DataFrame()\n",
    "    for i in range(df.select(pl.col('Metadata_Site')).max().item()):\n",
    "        df_slice = df.filter(pl.col('Metadata_Site')==i)\n",
    "        if arg == 'mean':\n",
    "            df_slice_agg = aggregate_mean(df_slice)\n",
    "            # df_slice_agg.write_parquet(f'{outdir}/{df.select(pl.col(\"Metadata_cmpd_cell_line\")).unique().item()}_Slice{i}MeanAgg.parquet')\n",
    "\n",
    "        elif arg == 'median':\n",
    "            df_slice_agg = aggregate_median(df_slice)\n",
    "            # df_slice_agg.write_parquet(f'{outdir}/{df.select(pl.col(\"Metadata_cmpd_cell_line\")).unique().item()}_Slice{i}MedianAgg.parquet')\n",
    "        else:\n",
    "            print('ERROR no valid metric')\n",
    "        del df_slice\n",
    "        df_sAgg = pl.concat([df_sAgg, df_slice_agg])\n",
    "    # df_sAgg.write_parquet(f'{outdir}/{df.select(pl.col(\"Metadata_cmpd_cell_line\")).unique().item()}_Slices_{arg}Agg.parquet')\n",
    "    return df_sAgg\n",
    "\n",
    "def data_processing(metaEx, cl, ObjectList, OutputDir, sliceLim=13, statmet='minmax', per_plate=False):\n",
    "    OutputDir = f'{OutputDir}_{statmet}_bulkstd'\n",
    "    if per_plate:\n",
    "        OutputDir = f'{OutputDir}_PerPlate'\n",
    "    #Filtering Metadata and generating dirlist\n",
    "    metaEx = metaEx.filter(pl.col('cell_line')==cl)\n",
    "    barcodes = metaEx.select(pl.col('barcode')).unique()\n",
    "    barcodes = barcodes['barcode']\n",
    "    barcodes.to_list()\n",
    "    dirlist = [f'{sourceDir}/{barcode}' for barcode in barcodes]\n",
    "\n",
    "    print('Starting Processing')\n",
    "    df = pl.DataFrame()\n",
    "    for BaseDir in dirlist:\n",
    "        mdf_op = metaEx.filter(pl.col('barcode') == BaseDir.split('/')[-1])\n",
    "        image_id = mdf_op.select(pl.col('image_id')).unique().to_series().to_list()[-1]\n",
    "        cp_id = mdf_op.select(pl.col('cp_id')).unique().to_series().to_list()[-1]\n",
    "        print(f'{BaseDir}/{image_id}/{cp_id}')\n",
    "        BaseDir = f'{BaseDir}/{image_id}/{cp_id}'\n",
    "\n",
    "\n",
    "        # nuclei = pl.read_parquet(BaseDir+f'/{ObjectList[0]}.parquet').add_prefix('Nuclei_').reset_index()\n",
    "        # cytoplasm = pl.read_parquet(BaseDir+f'/{ObjectList[1]}.parquet').add_prefix('Cytoplasm_').reset_index()\n",
    "        # cells = pl.read_parquet(BaseDir+f'/{ObjectList[2]}.parquet').add_prefix('Cells_').reset_index()\n",
    "        # f_df=pl.read_parquet(f)\n",
    "        # f_df=f_df.rename({x:f'{feature_set_name}_{x}' for x in f_df.columns})\n",
    "        \n",
    "\n",
    "        nuclei = pl.read_parquet(BaseDir+f'/{ObjectList[0]}.parquet')\n",
    "        nuclei=nuclei.rename({x:f'Nuclei_{x}' for x in nuclei.columns})\n",
    "        cytoplasm = pl.read_parquet(BaseDir+f'/{ObjectList[1]}.parquet')\n",
    "        cytoplasm=cytoplasm.rename({x:f'Cytoplasm_{x}' for x in cytoplasm.columns})\n",
    "        cells = pl.read_parquet(BaseDir+f'/{ObjectList[2]}.parquet')\n",
    "        cells=cells.rename({x:f'Cells_{x}' for x in cells.columns})\n",
    "        # step 1: Take the mean values of 'multiple nuclei' belonging to one cell\n",
    "\n",
    "        nuclei = nuclei.group_by([\n",
    "            \"Nuclei_Metadata_Barcode\",\"Nuclei_Metadata_Well\",\n",
    "            \"Nuclei_Parent_cells\", 'Nuclei_Metadata_Site'\n",
    "        ]).mean()\n",
    "\n",
    "        df_one = cytoplasm.join(nuclei,\n",
    "                    how='left', \n",
    "                    right_on=['Nuclei_Metadata_Well', 'Nuclei_Metadata_Site', 'Nuclei_Parent_cells', 'Nuclei_Metadata_Barcode'],\n",
    "                    left_on = ['Cytoplasm_Metadata_Well','Cytoplasm_Metadata_Site', 'Cytoplasm_ObjectNumber', 'Cytoplasm_Metadata_Barcode'])\n",
    "                    \n",
    "        df_one = df_one.join(cells, how='left', \n",
    "                        left_on=['Cytoplasm_Metadata_Well','Cytoplasm_Metadata_Site','Cytoplasm_ObjectNumber', 'Cytoplasm_Metadata_Barcode'],\n",
    "                        right_on = ['Cells_Metadata_Well','Cells_Metadata_Site',\"Cells_ObjectNumber\", 'Cells_Metadata_Barcode'])\n",
    "\n",
    "        # print_time(\"initial merging\")\n",
    "        print('part1')\n",
    "\n",
    "        # deduplicate barcode/well/site - renamed cytoplasm_Metadata* to Metadata* and removes nuclei_* etc\n",
    "        unique_metadata_feature_names=['Metadata_Barcode','Metadata_Well','Metadata_Site']\n",
    "        df_one=df_one.rename({f'Cytoplasm_{suffix}':suffix for suffix in unique_metadata_feature_names})\n",
    "        # df = df.filter(pl.col(''))       \n",
    "        # for some reason, the site is parsed as float, even though it really should be an int\n",
    "        if df_one['Metadata_Site'].dtype in [np.dtype('float32'), np.dtype('float64')]:\n",
    "            # sometimes, for some reason, site indices are inf/nan\n",
    "            site_is_nan_mask=np.isnan(df_one['Metadata_Site'])\n",
    "            site_is_inf_mask=np.isinf(df_one['Metadata_Site'])\n",
    "            \n",
    "            try:\n",
    "                num_sites_nan=np.sum(site_is_nan_mask)\n",
    "                num_sites_inf=np.sum(site_is_inf_mask)\n",
    "                assert num_sites_nan==0, f\"found nan site values (n = {num_sites_nan})\"\n",
    "                assert num_sites_inf==0, f\"found inf site values (n = {num_sites_inf})\"\n",
    "            except AssertionError as e:\n",
    "                print(f\"info - this issue was automatically circumvented in the code : {e}\")\n",
    "                df_one=df_one[~(site_is_inf_mask|site_is_nan_mask)]\n",
    "                \n",
    "            num_metadata_site_entries_nonint=np.sum(np.abs(df_one['Metadata_Site']%1.0)>1e-6)\n",
    "            assert num_metadata_site_entries_nonint==0, f\"ERROR : {num_metadata_site_entries_nonint} imaging sites don't have integer indices. that should not be the case, and likely indicates a bug.\"\n",
    "            \n",
    "            #Should use np.round, no? TODO ask patrick. Truncation Errors are annoying.\n",
    "            df_one['Metadata_Site'] = df_one['Metadata_Site'].astype(np.dtype('int32'))\n",
    "        \n",
    "        #Adding Compound Metadata to each row\n",
    "        df_one = df_one.join(mdf_op.rename({x:f\"Metadata_cmpd_{x}\" for x in mdf_op.columns}),left_on='Metadata_Well',right_on='Metadata_cmpd_well_id')\n",
    "        df_one = df_one.filter(pl.col('Metadata_Site')<sliceLim)\n",
    "        df = pl.concat([df, df_one])\n",
    "        plate_name = f'processed metadata for {BaseDir.split(\"/\")[-1]}'\n",
    "        print(plate_name)\n",
    "    df = df.sort(pl.col('Metadata_Site'))\n",
    "    ###Here should be workable to unify by cell line.\n",
    "    df = df.with_columns((pl.col(\"Metadata_Barcode\") + \"_\" + pl.col(\"Metadata_Well\")).alias(\"Metadata_PlateWell\"))\n",
    "    print(df.select(pl.col('Metadata_cmpd_cell_line')).to_series().unique().to_list())\n",
    "    df = df.filter(pl.col('Metadata_cmpd_cell_line')==cl)\n",
    "    print(df.select(pl.col('Metadata_cmpd_cell_line')).to_series().unique().to_list())\n",
    "    ###End\n",
    "    # drop all rows that contain nan\n",
    "    num_rows_before_nan_trim = df.shape[0]\n",
    "    for col in df.select([pl.col(pl.Float32),pl.col(pl.Float64)]).columns:\n",
    "        before_drop=df.shape[0]\n",
    "        df=df.filter(pl.col(col).is_not_null())\n",
    "        after_drop=df.shape[0]\n",
    "\n",
    "        num_values_dropped=before_drop-after_drop\n",
    "        if num_values_dropped>0:\n",
    "           print(f\"dropped {num_values_dropped} rows due to NaNs in column {col}\")\n",
    "\n",
    "    num_rows_after_nan_trim = df.shape[0]\n",
    "    print_time(\"dropped NaNs\")\n",
    "    # Clip outliers\n",
    "    use_clipping = False\n",
    "    if use_clipping:\n",
    "        print('clipping values....')\n",
    "        float_cols = [c for c_name,c_dtype in zip(df.columns,df.dtypes) if \"float\" in str(c_dtype)]\n",
    "        lower_quantile = df.select(float_cols).quantile(0.01)\n",
    "        upper_quantile = df.select(float_cols).quantile(0.99)\n",
    "        print(\"calced quantiles\")\n",
    "        for col in float_cols:\n",
    "            df = df.with_column(\n",
    "                pl.col(col).clip(lower=lower_quantile[col],upper=upper_quantile[col])\n",
    "            )\n",
    "        print(\"clipped\")\n",
    "\n",
    "    # # # df_mean=df.with_columns([df_standardized[c] for c in df_standardized.columns])\n",
    "\n",
    "    \n",
    "    # ScOut = f'{OutputDir}/SingleCell/'\n",
    "    # if not os.path.exists(ScOut): \n",
    "    #     os.makedirs(ScOut)\n",
    "    # df.write_parquet(f'{ScOut}/{df.select(pl.col(\"Metadata_cmpd_cell_line\")).unique().item()}.parquet')\n",
    "\n",
    "    # if make_slices:\n",
    "    slicesOut = f'{OutputDir}/SingleSlice/'\n",
    "    arg_slice = 'median'\n",
    "    if not os.path.exists(slicesOut): \n",
    "        os.makedirs(slicesOut)\n",
    "    df = generate_slices(df, slicesOut, arg_slice)\n",
    "    print(f'Standardizing per pate:    {per_plate}')\n",
    "    if statmet == 'meanstd':\n",
    "        print('Standardizing by MeanSTD on a per slice basis only')\n",
    "        if per_plate:\n",
    "            print('standardizing per plate and per slice')\n",
    "            df = standardize_mean_perplate(df)\n",
    "        else:\n",
    "            df = standardize_mean_noslice(df)\n",
    "    if statmet == 'minmax':\n",
    "        print('Normalizing to unit on a per slice only')\n",
    "        if per_plate:\n",
    "            print('normalizing to unit per plate and per slice')\n",
    "            df = normalize_perplate(df)\n",
    "        else:\n",
    "            df = normalize(df)\n",
    "    df.write_parquet(f'{slicesOut}/{df.select(pl.col(\"Metadata_cmpd_cell_line\")).unique().item()}_Slices_{arg_slice}Agg.parquet')\n",
    "    # df_mean = df\n",
    "    \n",
    "\n",
    "    #Generating the output directories\n",
    "    # aggOut = f'{OutputDir}/WellAggregates/'\n",
    "    # if not os.path.exists(aggOut): \n",
    "    #     os.makedirs(aggOut)\n",
    "    \n",
    "\n",
    "    # # df_agg_mean = aggregate_mean(df)\n",
    "    # # df_agg_mean.write_parquet(f'{aggOut}/{df.select(pl.col(\"Metadata_cmpd_cell_line\")).unique().item()}_MeanAgg_meanstd.parquet')\n",
    "    # df_agg_median = aggregate_median(df)\n",
    "    # df_agg_median.write_parquet(f'{aggOut}/{df.select(pl.col(\"Metadata_cmpd_cell_line\")).unique().item()}_MedianAgg_meanstd.parquet')\n",
    "    # del df_agg_median\n",
    "    # del df_agg_mean\n",
    "    # del df_mean\n",
    "    print_time(\"binned mean data per well\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaEx = pl.read_csv(f'{rootDir}/settings/spher-colo52-v1-import-inputFiles-and-PLAIDresults.csv')\n",
    "metaEx = metaEx.drop(cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>well_id</th><th>image_id</th><th>cp_id</th><th>barcode</th><th>plate_well</th><th>cmpdname</th><th>pert_type</th><th>target</th><th>pathway</th><th>target_type</th><th>cell_line</th></tr><tr><td>str</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;B02&quot;</td><td>4185</td><td>5532</td><td>&quot;PB000137&quot;</td><td>&quot;PB000137_B02&quot;</td><td>&quot;PD0325901&quot;</td><td>&quot;trt&quot;</td><td>&quot;MEK&quot;</td><td>&quot;MAPK&quot;</td><td>&quot;Targeted&quot;</td><td>&quot;HCT116&quot;</td></tr><tr><td>&quot;B03&quot;</td><td>4185</td><td>5532</td><td>&quot;PB000137&quot;</td><td>&quot;PB000137_B03&quot;</td><td>&quot;Paclitaxel&quot;</td><td>&quot;trt&quot;</td><td>&quot;Autophagy,Microtubule Associat…</td><td>&quot;Cytoskeletal Signaling&quot;</td><td>&quot;Cytotoxic&quot;</td><td>&quot;HCT116&quot;</td></tr><tr><td>&quot;B04&quot;</td><td>4185</td><td>5532</td><td>&quot;PB000137&quot;</td><td>&quot;PB000137_B04&quot;</td><td>&quot;Olaparib (AZD2281, Ku-0059436)&quot;</td><td>&quot;trt&quot;</td><td>&quot;PARP&quot;</td><td>&quot;DNA Damage&quot;</td><td>&quot;Targeted&quot;</td><td>&quot;HCT116&quot;</td></tr><tr><td>&quot;B05&quot;</td><td>4185</td><td>5532</td><td>&quot;PB000137&quot;</td><td>&quot;PB000137_B05&quot;</td><td>&quot;SB216763&quot;</td><td>&quot;trt&quot;</td><td>&quot;GSK-3&quot;</td><td>&quot;PI3K/Akt/mTOR&quot;</td><td>&quot;Targeted&quot;</td><td>&quot;HCT116&quot;</td></tr><tr><td>&quot;B06&quot;</td><td>4185</td><td>5532</td><td>&quot;PB000137&quot;</td><td>&quot;PB000137_B06&quot;</td><td>&quot;Vorinostat (SAHA, MK0683)&quot;</td><td>&quot;trt&quot;</td><td>&quot;Autophagy,HDAC&quot;</td><td>&quot;Epigenetics&quot;</td><td>&quot;Targeted&quot;</td><td>&quot;HCT116&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 11)\n",
       "┌─────────┬──────────┬───────┬──────────┬───┬──────────────┬─────────────┬─────────────┬───────────┐\n",
       "│ well_id ┆ image_id ┆ cp_id ┆ barcode  ┆ … ┆ target       ┆ pathway     ┆ target_type ┆ cell_line │\n",
       "│ ---     ┆ ---      ┆ ---   ┆ ---      ┆   ┆ ---          ┆ ---         ┆ ---         ┆ ---       │\n",
       "│ str     ┆ i64      ┆ i64   ┆ str      ┆   ┆ str          ┆ str         ┆ str         ┆ str       │\n",
       "╞═════════╪══════════╪═══════╪══════════╪═══╪══════════════╪═════════════╪═════════════╪═══════════╡\n",
       "│ B02     ┆ 4185     ┆ 5532  ┆ PB000137 ┆ … ┆ MEK          ┆ MAPK        ┆ Targeted    ┆ HCT116    │\n",
       "│ B03     ┆ 4185     ┆ 5532  ┆ PB000137 ┆ … ┆ Autophagy,Mi ┆ Cytoskeleta ┆ Cytotoxic   ┆ HCT116    │\n",
       "│         ┆          ┆       ┆          ┆   ┆ crotubule    ┆ l Signaling ┆             ┆           │\n",
       "│         ┆          ┆       ┆          ┆   ┆ Associat…    ┆             ┆             ┆           │\n",
       "│ B04     ┆ 4185     ┆ 5532  ┆ PB000137 ┆ … ┆ PARP         ┆ DNA Damage  ┆ Targeted    ┆ HCT116    │\n",
       "│ B05     ┆ 4185     ┆ 5532  ┆ PB000137 ┆ … ┆ GSK-3        ┆ PI3K/Akt/mT ┆ Targeted    ┆ HCT116    │\n",
       "│         ┆          ┆       ┆          ┆   ┆              ┆ OR          ┆             ┆           │\n",
       "│ B06     ┆ 4185     ┆ 5532  ┆ PB000137 ┆ … ┆ Autophagy,HD ┆ Epigenetics ┆ Targeted    ┆ HCT116    │\n",
       "│         ┆          ┆       ┆          ┆   ┆ AC           ┆             ┆             ┆           │\n",
       "└─────────┴──────────┴───────┴──────────┴───┴──────────────┴─────────────┴─────────────┴───────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metaEx.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadmso = metaEx.filter(pl.col('Compound_ID')=='DMSO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>well_id</th><th>image_id</th><th>cp_id</th><th>barcode</th><th>plate_well</th><th>cmpdname</th><th>pert_type</th><th>target</th><th>pathway</th><th>target_type</th><th>cell_line</th></tr><tr><td>str</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;B02&quot;</td><td>4185</td><td>5532</td><td>&quot;PB000137&quot;</td><td>&quot;PB000137_B02&quot;</td><td>&quot;PD0325901&quot;</td><td>&quot;trt&quot;</td><td>&quot;MEK&quot;</td><td>&quot;MAPK&quot;</td><td>&quot;Targeted&quot;</td><td>&quot;HCT116&quot;</td></tr><tr><td>&quot;B03&quot;</td><td>4185</td><td>5532</td><td>&quot;PB000137&quot;</td><td>&quot;PB000137_B03&quot;</td><td>&quot;Paclitaxel&quot;</td><td>&quot;trt&quot;</td><td>&quot;Autophagy,Microtubule Associat…</td><td>&quot;Cytoskeletal Signaling&quot;</td><td>&quot;Cytotoxic&quot;</td><td>&quot;HCT116&quot;</td></tr><tr><td>&quot;B04&quot;</td><td>4185</td><td>5532</td><td>&quot;PB000137&quot;</td><td>&quot;PB000137_B04&quot;</td><td>&quot;Olaparib (AZD2281, Ku-0059436)&quot;</td><td>&quot;trt&quot;</td><td>&quot;PARP&quot;</td><td>&quot;DNA Damage&quot;</td><td>&quot;Targeted&quot;</td><td>&quot;HCT116&quot;</td></tr><tr><td>&quot;B05&quot;</td><td>4185</td><td>5532</td><td>&quot;PB000137&quot;</td><td>&quot;PB000137_B05&quot;</td><td>&quot;SB216763&quot;</td><td>&quot;trt&quot;</td><td>&quot;GSK-3&quot;</td><td>&quot;PI3K/Akt/mTOR&quot;</td><td>&quot;Targeted&quot;</td><td>&quot;HCT116&quot;</td></tr><tr><td>&quot;B06&quot;</td><td>4185</td><td>5532</td><td>&quot;PB000137&quot;</td><td>&quot;PB000137_B06&quot;</td><td>&quot;Vorinostat (SAHA, MK0683)&quot;</td><td>&quot;trt&quot;</td><td>&quot;Autophagy,HDAC&quot;</td><td>&quot;Epigenetics&quot;</td><td>&quot;Targeted&quot;</td><td>&quot;HCT116&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 11)\n",
       "┌─────────┬──────────┬───────┬──────────┬───┬──────────────┬─────────────┬─────────────┬───────────┐\n",
       "│ well_id ┆ image_id ┆ cp_id ┆ barcode  ┆ … ┆ target       ┆ pathway     ┆ target_type ┆ cell_line │\n",
       "│ ---     ┆ ---      ┆ ---   ┆ ---      ┆   ┆ ---          ┆ ---         ┆ ---         ┆ ---       │\n",
       "│ str     ┆ i64      ┆ i64   ┆ str      ┆   ┆ str          ┆ str         ┆ str         ┆ str       │\n",
       "╞═════════╪══════════╪═══════╪══════════╪═══╪══════════════╪═════════════╪═════════════╪═══════════╡\n",
       "│ B02     ┆ 4185     ┆ 5532  ┆ PB000137 ┆ … ┆ MEK          ┆ MAPK        ┆ Targeted    ┆ HCT116    │\n",
       "│ B03     ┆ 4185     ┆ 5532  ┆ PB000137 ┆ … ┆ Autophagy,Mi ┆ Cytoskeleta ┆ Cytotoxic   ┆ HCT116    │\n",
       "│         ┆          ┆       ┆          ┆   ┆ crotubule    ┆ l Signaling ┆             ┆           │\n",
       "│         ┆          ┆       ┆          ┆   ┆ Associat…    ┆             ┆             ┆           │\n",
       "│ B04     ┆ 4185     ┆ 5532  ┆ PB000137 ┆ … ┆ PARP         ┆ DNA Damage  ┆ Targeted    ┆ HCT116    │\n",
       "│ B05     ┆ 4185     ┆ 5532  ┆ PB000137 ┆ … ┆ GSK-3        ┆ PI3K/Akt/mT ┆ Targeted    ┆ HCT116    │\n",
       "│         ┆          ┆       ┆          ┆   ┆              ┆ OR          ┆             ┆           │\n",
       "│ B06     ┆ 4185     ┆ 5532  ┆ PB000137 ┆ … ┆ Autophagy,HD ┆ Epigenetics ┆ Targeted    ┆ HCT116    │\n",
       "│         ┆          ┆       ┆          ┆   ┆ AC           ┆             ┆             ┆           │\n",
       "└─────────┴──────────┴───────┴──────────┴───┴──────────────┴─────────────┴─────────────┴───────────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metaEx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HT29', 'HCT116']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metaEx.select(pl.col(['cell_line'])).unique().to_series().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HCT116', 'HT29']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metaEx.select(pl.col(['cell_line'])).unique().to_series().to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObjectList = ['featICF_nuclei', 'featICF_cells', 'featICF_cytoplasm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PB000140', 'PB000138', 'PB000141', 'PB000142', 'PB000139', 'PB000137']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metaEx.select(pl.col(['barcode'])).unique().to_series().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_processing(metaEx, 'HCT116', ObjectList, OutputDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Processing\n",
      "/share/data/cellprofiler/automation/results/PB000137/4185/5532\n",
      "part1\n",
      "processed metadata for 5532\n",
      "/share/data/cellprofiler/automation/results/PB000140/4189/5573\n",
      "part1\n",
      "processed metadata for 5573\n",
      "/share/data/cellprofiler/automation/results/PB000139/4186/5533\n",
      "part1\n",
      "processed metadata for 5533\n",
      "/share/data/cellprofiler/automation/results/PB000138/4188/5547\n",
      "part1\n",
      "processed metadata for 5547\n",
      "['HCT116']\n",
      "['HCT116']\n",
      "dropped 1 rows due to NaNs in column Cytoplasm_AreaShape_FormFactor\n",
      "dropped 3 rows due to NaNs in column Cytoplasm_Correlation_Costes_CONC_HOECHST\n",
      "dropped 1 rows due to NaNs in column Cytoplasm_Correlation_Costes_MITO_HOECHST\n",
      "dropped 25 rows due to NaNs in column Cytoplasm_Neighbors_AngleBetweenNeighbors_Adjacent\n",
      "dropped 4 rows due to NaNs in column Cytoplasm_Neighbors_FirstClosestDistance_Adjacent\n",
      "dropped 36 rows due to NaNs in column Nuclei_Neighbors_AngleBetweenNeighbors_Adjacent\n",
      "dropped 4 rows due to NaNs in column Nuclei_Neighbors_FirstClosestDistance_Adjacent\n",
      "dropped 30 rows due to NaNs in column Cells_AreaShape_Area\n",
      "dropped 11 rows due to NaNs in column Cells_AreaShape_FormFactor\n",
      "dropped 28 rows due to NaNs in column Cells_AreaShape_Zernike_0_0\n",
      "2024-10-24 10:09:49 dropped NaNs\n",
      "Standardizing per pate:    True\n",
      "Standardizing by MeanSTD on a per slice basis only\n",
      "standardizing per plate and per slice\n",
      "['PB000137', 'PB000140', 'PB000139', 'PB000138']\n",
      "processing barcode PB000137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_321141/1324017831.py:72: DeprecationWarning: The `default` parameter for `replace` is deprecated. Use `replace_strict` instead to set a default while replacing values.\n",
      "  std = std.select([pl.col(c).replace({0: 1}, default=pl.col(c)) for c in std.columns])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-24 10:09:56 calculated DMSO distribution\n",
      "processing barcode PB000140\n",
      "2024-10-24 10:09:57 calculated DMSO distribution\n",
      "processing barcode PB000139\n",
      "2024-10-24 10:09:57 calculated DMSO distribution\n",
      "processing barcode PB000138\n",
      "2024-10-24 10:09:58 calculated DMSO distribution\n",
      "2024-10-24 10:09:59 binned mean data per well\n"
     ]
    }
   ],
   "source": [
    "data_processing(metaEx, 'HCT116', ObjectList, OutputDir, statmet='meanstd', per_plate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Processing\n",
      "/share/data/cellprofiler/automation/results/PB000142/4191/5574\n",
      "part1\n",
      "processed metadata for 5574\n",
      "/share/data/cellprofiler/automation/results/PB000139/4186/5533\n",
      "part1\n",
      "processed metadata for 5533\n",
      "/share/data/cellprofiler/automation/results/PB000141/4187/5546\n",
      "part1\n",
      "processed metadata for 5546\n",
      "/share/data/cellprofiler/automation/results/PB000140/4189/5573\n",
      "part1\n",
      "processed metadata for 5573\n",
      "['HT29']\n",
      "['HT29']\n",
      "dropped 1 rows due to NaNs in column Cytoplasm_Correlation_Costes_CONC_HOECHST\n",
      "dropped 26 rows due to NaNs in column Cytoplasm_Neighbors_AngleBetweenNeighbors_Adjacent\n",
      "dropped 2 rows due to NaNs in column Cytoplasm_Neighbors_FirstClosestDistance_Adjacent\n",
      "dropped 53 rows due to NaNs in column Nuclei_Neighbors_AngleBetweenNeighbors_Adjacent\n",
      "dropped 2 rows due to NaNs in column Nuclei_Neighbors_FirstClosestDistance_Adjacent\n",
      "dropped 28 rows due to NaNs in column Cells_AreaShape_Area\n",
      "dropped 20 rows due to NaNs in column Cells_AreaShape_FormFactor\n",
      "dropped 25 rows due to NaNs in column Cells_AreaShape_Zernike_0_0\n",
      "2024-10-24 10:12:12 dropped NaNs\n",
      "Standardizing per pate:    True\n",
      "Standardizing by MeanSTD on a per slice basis only\n",
      "standardizing per plate and per slice\n",
      "['PB000142', 'PB000139', 'PB000141', 'PB000140']\n",
      "processing barcode PB000142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_321141/1324017831.py:72: DeprecationWarning: The `default` parameter for `replace` is deprecated. Use `replace_strict` instead to set a default while replacing values.\n",
      "  std = std.select([pl.col(c).replace({0: 1}, default=pl.col(c)) for c in std.columns])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-24 10:12:17 calculated DMSO distribution\n",
      "processing barcode PB000139\n",
      "2024-10-24 10:12:18 calculated DMSO distribution\n",
      "processing barcode PB000141\n",
      "2024-10-24 10:12:19 calculated DMSO distribution\n",
      "processing barcode PB000140\n",
      "2024-10-24 10:12:19 calculated DMSO distribution\n",
      "2024-10-24 10:12:21 binned mean data per well\n"
     ]
    }
   ],
   "source": [
    "data_processing(metaEx, 'HT29', ObjectList, OutputDir, statmet='meanstd', per_plate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current date and time : \n",
      "2024-10-24 10:12:21\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "print ('Current date and time : ')\n",
    "print (now.strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5574"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metaEx.select(pl.col('cp_id')).max().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
